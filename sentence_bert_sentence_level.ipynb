{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence BERT — 문장 단위 분석\n",
    "\n",
    "각 Abstract를 **문장 단위**로 분해한 뒤 Sentence BERT 임베딩을 적용합니다.\n",
    "\n",
    "| 단계 | 내용 |\n",
    "|------|------|\n",
    "| 1 | 데이터 로드 & 연도 선택 |\n",
    "| 2 | Kiwi로 문장 분리 |\n",
    "| 3 | Sentence BERT 임베딩 |\n",
    "| 4 | UMAP 시각화 |\n",
    "| 5 | HDBSCAN 클러스터링 |\n",
    "| 6 | 클러스터별 키워드 (TF-IDF) |\n",
    "| 7 | 인터랙티브 시각화 (Plotly) |\n",
    "| 8 | 문서 수준 집계 (문장 → 논문) |\n",
    "| 9 | 유사 문장 검색 |\n",
    "\n",
    "**모델**: `snunlp/KR-SBERT-V40K-klueNLI-augSTS`  \n",
    "**문장 분리**: `kiwipiepy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라이브러리 로드 완료\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import platform, re\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "else:\n",
    "    plt.rcParams['font.family'] = 'NanumGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "DATA_PATH = Path('abstract_year_data.csv')\n",
    "print('라이브러리 로드 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 & 연도 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터: 2,664건\n",
      "연도 범위: 2018 ~ 2025\n",
      "year\n",
      "2018      36\n",
      "2019      42\n",
      "2020      73\n",
      "2021     134\n",
      "2022     177\n",
      "2023     294\n",
      "2024     531\n",
      "2025    1377\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(DATA_PATH, encoding='utf-8-sig')\n",
    "df_raw = df_raw.dropna(subset=['abstract']).copy()\n",
    "df_raw['abstract'] = df_raw['abstract'].astype(str).str.strip()\n",
    "df_raw = df_raw[df_raw['abstract'].str.len() > 20].reset_index(drop=True)\n",
    "\n",
    "print(f'전체 데이터: {len(df_raw):,}건')\n",
    "print(f'연도 범위: {df_raw[\"year\"].min()} ~ {df_raw[\"year\"].max()}')\n",
    "print(df_raw['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7defc21c64a946458e33a15004af6d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(RadioButtons(description='선택 방식:', layout=Layout(width='320px'), options=('연도 범위 선택', '특정 연도 선택…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "_all_years = sorted(df_raw['year'].dropna().unique().astype(int).tolist())\n",
    "_year_min, _year_max = min(_all_years), max(_all_years)\n",
    "\n",
    "_mode = widgets.RadioButtons(\n",
    "    options=['연도 범위 선택', '특정 연도 선택'],\n",
    "    value='연도 범위 선택',\n",
    "    description='선택 방식:',\n",
    "    style={'description_width': '90px'},\n",
    "    layout=widgets.Layout(width='320px')\n",
    ")\n",
    "\n",
    "_slider = widgets.IntRangeSlider(\n",
    "    value=[_year_min, _year_max],\n",
    "    min=_year_min, max=_year_max, step=1,\n",
    "    description='연도 범위:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '90px'},\n",
    "    layout=widgets.Layout(width='480px')\n",
    ")\n",
    "\n",
    "_multi = widgets.SelectMultiple(\n",
    "    options=_all_years,\n",
    "    value=_all_years,\n",
    "    description='연도 선택:',\n",
    "    rows=min(len(_all_years), 10),\n",
    "    style={'description_width': '90px'},\n",
    "    layout=widgets.Layout(width='320px', height='180px', display='none')\n",
    ")\n",
    "\n",
    "_apply_btn = widgets.Button(description='적용', button_style='success',\n",
    "                             layout=widgets.Layout(width='90px', margin='8px 0 0 0'))\n",
    "_reset_btn = widgets.Button(description='전체 초기화', button_style='warning',\n",
    "                             layout=widgets.Layout(width='110px', margin='8px 0 0 8px'))\n",
    "_out = widgets.Output()\n",
    "\n",
    "# 전역 변수 초기화\n",
    "df = df_raw.copy()\n",
    "SENT_CACHE    = Path('sent_embeddings_all.npy')\n",
    "SENT_META     = Path('sent_meta_all.csv')\n",
    "UMAP_CACHE_2D = Path('sent_umap_2d_all.npy')\n",
    "\n",
    "def _on_mode(change):\n",
    "    if change['new'] == '연도 범위 선택':\n",
    "        _slider.layout.display = ''\n",
    "        _multi.layout.display = 'none'\n",
    "    else:\n",
    "        _slider.layout.display = 'none'\n",
    "        _multi.layout.display = ''\n",
    "\n",
    "_mode.observe(_on_mode, names='value')\n",
    "\n",
    "def _apply(b):\n",
    "    global df, SENT_CACHE, SENT_META, UMAP_CACHE_2D\n",
    "    with _out:\n",
    "        clear_output()\n",
    "        if _mode.value == '연도 범위 선택':\n",
    "            y1, y2 = _slider.value\n",
    "            selected = [y for y in _all_years if y1 <= y <= y2]\n",
    "        else:\n",
    "            selected = sorted(list(_multi.value))\n",
    "\n",
    "        df = df_raw[df_raw['year'].isin(selected)].reset_index(drop=True)\n",
    "        tag = f'{selected[0]}_{selected[-1]}'\n",
    "        SENT_CACHE    = Path(f'sent_embeddings_{tag}.npy')\n",
    "        SENT_META     = Path(f'sent_meta_{tag}.csv')\n",
    "        UMAP_CACHE_2D = Path(f'sent_umap_2d_{tag}.npy')\n",
    "\n",
    "        print(f'선택 연도: {selected[0]} ~ {selected[-1]}  ({len(selected)}개 연도)')\n",
    "        print(f'논문 수: {len(df):,}건')\n",
    "        print(f'캐시 파일: {SENT_CACHE.name}')\n",
    "        print()\n",
    "        print(df['year'].value_counts().sort_index().rename('건수').to_frame().T.to_string())\n",
    "        print('아래 셀들을 순서대로 실행하세요.')\n",
    "\n",
    "def _reset(b):\n",
    "    global df, SENT_CACHE, SENT_META, UMAP_CACHE_2D\n",
    "    with _out:\n",
    "        clear_output()\n",
    "        df = df_raw.copy()\n",
    "        SENT_CACHE    = Path('sent_embeddings_all.npy')\n",
    "        SENT_META     = Path('sent_meta_all.csv')\n",
    "        UMAP_CACHE_2D = Path('sent_umap_2d_all.npy')\n",
    "        _slider.value = [_year_min, _year_max]\n",
    "        _multi.value = _all_years\n",
    "        print(f'전체 초기화: {len(df):,}건')\n",
    "\n",
    "_apply_btn.on_click(_apply)\n",
    "_reset_btn.on_click(_reset)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    _mode, _slider, _multi,\n",
    "    widgets.HBox([_apply_btn, _reset_btn]),\n",
    "    _out\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 문장 분리 (Kiwi)\n",
    "\n",
    "각 abstract를 `kiwipiepy`로 문장 단위로 분리합니다.  \n",
    "결과는 `df_sent` (문장 단위 DataFrame)에 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiwi 문장 분리 시작... (논문 2,586건)\n",
      "  처리: 500/2586\n",
      "  처리: 1000/2586\n",
      "  처리: 1500/2586\n",
      "  처리: 2000/2586\n",
      "  처리: 2500/2586\n",
      "완료: 21,509개 문장  (sent_meta_2020_2025.csv 저장)\n",
      "\n",
      "논문 1건당 평균 문장 수: 8.3개\n",
      "최소 문장 길이: 11자\n",
      "최대 문장 길이: 4150자\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_idx</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>year</th>\n",
       "      <th>sentence</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>본 연구는 2015 개정 영어교육과정의 강조점인 과정중심평가의 맥락에서 인공지능(a...</td>\n",
       "      <td>본 연구는 2015 개정 영어교육과정의 강조점인 과정중심평가의 맥락에서 인공지능(a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>연구자는 서울의 한 초등학교의 3학년 학생들을 대상으로 AI챗봇을 프로그래밍하여 과...</td>\n",
       "      <td>본 연구는 2015 개정 영어교육과정의 강조점인 과정중심평가의 맥락에서 인공지능(a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>실험 결과, 먼저, AI챗봇의 특정한 기능(‘History’)이 과정중심 말하기 평...</td>\n",
       "      <td>본 연구는 2015 개정 영어교육과정의 강조점인 과정중심평가의 맥락에서 인공지능(a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>이 기능은 평가의 결과뿐만 아니라 그 과정을 확인할 수 있어, 교사가 향후 학생들의...</td>\n",
       "      <td>본 연구는 2015 개정 영어교육과정의 강조점인 과정중심평가의 맥락에서 인공지능(a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2020</td>\n",
       "      <td>둘째, 3학년 학생들에게 행한 설문조사결과, 학생들은 대체적으로 AI챗봇과 상호작용...</td>\n",
       "      <td>본 연구는 2015 개정 영어교육과정의 강조점인 과정중심평가의 맥락에서 인공지능(a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_idx  sent_idx  year                                           sentence  \\\n",
       "0        0         0  2020  본 연구는 2015 개정 영어교육과정의 강조점인 과정중심평가의 맥락에서 인공지능(a...   \n",
       "1        0         1  2020  연구자는 서울의 한 초등학교의 3학년 학생들을 대상으로 AI챗봇을 프로그래밍하여 과...   \n",
       "2        0         2  2020  실험 결과, 먼저, AI챗봇의 특정한 기능(‘History’)이 과정중심 말하기 평...   \n",
       "3        0         3  2020  이 기능은 평가의 결과뿐만 아니라 그 과정을 확인할 수 있어, 교사가 향후 학생들의...   \n",
       "4        0         4  2020  둘째, 3학년 학생들에게 행한 설문조사결과, 학생들은 대체적으로 AI챗봇과 상호작용...   \n",
       "\n",
       "                                            abstract  \n",
       "0  본 연구는 2015 개정 영어교육과정의 강조점인 과정중심평가의 맥락에서 인공지능(a...  \n",
       "1  본 연구는 2015 개정 영어교육과정의 강조점인 과정중심평가의 맥락에서 인공지능(a...  \n",
       "2  본 연구는 2015 개정 영어교육과정의 강조점인 과정중심평가의 맥락에서 인공지능(a...  \n",
       "3  본 연구는 2015 개정 영어교육과정의 강조점인 과정중심평가의 맥락에서 인공지능(a...  \n",
       "4  본 연구는 2015 개정 영어교육과정의 강조점인 과정중심평가의 맥락에서 인공지능(a...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# 캐시 확인\n",
    "if SENT_META.exists():\n",
    "    df_sent = pd.read_csv(SENT_META, encoding='utf-8-sig')\n",
    "    print(f'캐시 로드: {SENT_META.name}  ({len(df_sent):,}개 문장)')\n",
    "else:\n",
    "    kiwi = Kiwi()\n",
    "    print(f'Kiwi 문장 분리 시작... (논문 {len(df):,}건)')\n",
    "\n",
    "    rows = []\n",
    "    for doc_idx, row in df.iterrows():\n",
    "        try:\n",
    "            sents = kiwi.split_into_sents(row['abstract'])\n",
    "            for s_idx, s in enumerate(sents):\n",
    "                text = s.text.strip()\n",
    "                # 너무 짧은 문장(10자 미만) 제외\n",
    "                if len(text) < 10:\n",
    "                    continue\n",
    "                rows.append({\n",
    "                    'doc_idx': doc_idx,\n",
    "                    'sent_idx': s_idx,\n",
    "                    'year': row['year'],\n",
    "                    'sentence': text,\n",
    "                    'abstract': row['abstract'][:100]  # 원본 abstract 앞부분\n",
    "                })\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if (doc_idx + 1) % 500 == 0:\n",
    "            print(f'  처리: {doc_idx+1}/{len(df)}')\n",
    "\n",
    "    df_sent = pd.DataFrame(rows).reset_index(drop=True)\n",
    "    df_sent.to_csv(SENT_META, index=False, encoding='utf-8-sig')\n",
    "    print(f'완료: {len(df_sent):,}개 문장  ({SENT_META.name} 저장)')\n",
    "\n",
    "print(f'\\n논문 1건당 평균 문장 수: {len(df_sent)/len(df):.1f}개')\n",
    "print(f'최소 문장 길이: {df_sent[\"sentence\"].str.len().min()}자')\n",
    "print(f'최대 문장 길이: {df_sent[\"sentence\"].str.len().max()}자')\n",
    "df_sent.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentence BERT 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로드: snunlp/KR-SBERT-V40K-klueNLI-augSTS\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL_NAME = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "print(f'모델 로드: {MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 시작: 21,509개 문장...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d85bfe9f80416c833949b80908b576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if SENT_CACHE.exists():\n",
    "    sent_embeddings = np.load(SENT_CACHE)\n",
    "    print(f'캐시 로드: {SENT_CACHE.name}  {sent_embeddings.shape}')\n",
    "else:\n",
    "    sentences = df_sent['sentence'].tolist()\n",
    "    print(f'임베딩 시작: {len(sentences):,}개 문장...')\n",
    "    sent_embeddings = model.encode(\n",
    "        sentences,\n",
    "        batch_size=128,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    np.save(SENT_CACHE, sent_embeddings)\n",
    "    print(f'완료 & 저장: {sent_embeddings.shape}')\n",
    "\n",
    "print(f'임베딩 차원: {sent_embeddings.shape}  (문장 수 x 768)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. UMAP 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "if UMAP_CACHE_2D.exists():\n",
    "    umap_2d = np.load(UMAP_CACHE_2D)\n",
    "    print(f'캐시 로드: {UMAP_CACHE_2D.name}')\n",
    "else:\n",
    "    print('UMAP 2D 차원 축소 중...')\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=15,\n",
    "        min_dist=0.1,\n",
    "        metric='cosine',\n",
    "        random_state=42\n",
    "    )\n",
    "    umap_2d = reducer.fit_transform(sent_embeddings)\n",
    "    np.save(UMAP_CACHE_2D, umap_2d)\n",
    "    print('완료')\n",
    "\n",
    "df_sent['umap_x'] = umap_2d[:, 0]\n",
    "df_sent['umap_y'] = umap_2d[:, 1]\n",
    "print(f'UMAP 좌표 추가 완료: {umap_2d.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HDBSCAN 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=30,\n",
    "    min_samples=10,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "cluster_labels = clusterer.fit_predict(umap_2d)\n",
    "\n",
    "df_sent['cluster'] = cluster_labels\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "noise_ratio = (cluster_labels == -1).sum() / len(cluster_labels) * 100\n",
    "\n",
    "print(f'클러스터 수: {n_clusters}')\n",
    "print(f'노이즈 비율: {noise_ratio:.1f}%')\n",
    "print('클러스터별 문장 수:')\n",
    "print(pd.Series(cluster_labels).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 클러스터별 키워드 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "valid_clusters = [c for c in sorted(set(cluster_labels)) if c != -1]\n",
    "\n",
    "cluster_docs = {}\n",
    "for c in valid_clusters:\n",
    "    texts = df_sent[df_sent['cluster'] == c]['sentence'].tolist()\n",
    "    cluster_docs[c] = ' '.join(texts)\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=lambda t: re.findall(r'[가-힣]{2,}', t),\n",
    "    max_features=5000,\n",
    "    min_df=1\n",
    ")\n",
    "corpus = [cluster_docs[c] for c in valid_clusters]\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "print('클러스터별 상위 키워드 (TF-IDF):')\n",
    "print('='*70)\n",
    "cluster_keywords = {}\n",
    "for idx, c in enumerate(valid_clusters):\n",
    "    row = tfidf_matrix[idx].toarray().flatten()\n",
    "    top_idx = row.argsort()[::-1][:10]\n",
    "    keywords = [feature_names[i] for i in top_idx if row[i] > 0]\n",
    "    cluster_keywords[c] = keywords\n",
    "    n_sents = (df_sent['cluster'] == c).sum()\n",
    "    print(f'클러스터 {c:2d} ({n_sents:5,}문장): {\" | \".join(keywords[:8])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# 좌: 클러스터별\n",
    "ax = axes[0]\n",
    "unique_clusters = sorted(set(cluster_labels))\n",
    "cmap = plt.cm.get_cmap('tab20', max(len(unique_clusters), 1))\n",
    "for i, c in enumerate(unique_clusters):\n",
    "    mask = df_sent['cluster'] == c\n",
    "    color = 'lightgray' if c == -1 else cmap(i)\n",
    "    alpha = 0.2 if c == -1 else 0.6\n",
    "    label = '노이즈' if c == -1 else f'클러스터 {c}'\n",
    "    ax.scatter(df_sent.loc[mask, 'umap_x'], df_sent.loc[mask, 'umap_y'],\n",
    "               c=[color], alpha=alpha, s=5, linewidths=0, label=label)\n",
    "\n",
    "ax.set_title('HDBSCAN 클러스터링 (문장 단위)', fontsize=13)\n",
    "ax.set_xlabel('UMAP 1'); ax.set_ylabel('UMAP 2')\n",
    "if n_clusters <= 15:\n",
    "    ax.legend(markerscale=3, fontsize=7, loc='best')\n",
    "\n",
    "# 우: 연도별\n",
    "ax2 = axes[1]\n",
    "sc = ax2.scatter(df_sent['umap_x'], df_sent['umap_y'],\n",
    "                 c=df_sent['year'], cmap='plasma', alpha=0.3, s=5, linewidths=0)\n",
    "plt.colorbar(sc, ax=ax2, label='연도')\n",
    "ax2.set_title('연도별 분포 (문장 단위)', fontsize=13)\n",
    "ax2.set_xlabel('UMAP 1'); ax2.set_ylabel('UMAP 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sent_umap_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('sent_umap_visualization.png 저장')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df_sent['cluster_str'] = df_sent['cluster'].astype(str)\n",
    "df_sent['sent_short'] = df_sent['sentence'].str[:120] + '...'\n",
    "df_sent['keywords'] = df_sent['cluster'].map(\n",
    "    lambda c: ' / '.join(cluster_keywords.get(c, ['노이즈'])[:5])\n",
    ")\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_sent,\n",
    "    x='umap_x', y='umap_y',\n",
    "    color='cluster_str',\n",
    "    hover_data={'sent_short': True, 'year': True, 'keywords': True,\n",
    "                'umap_x': False, 'umap_y': False, 'cluster_str': False},\n",
    "    title='Sentence BERT — 문장 단위 UMAP + HDBSCAN',\n",
    "    labels={'cluster_str': '클러스터', 'umap_x': 'UMAP 1', 'umap_y': 'UMAP 2'},\n",
    "    opacity=0.5, width=950, height=670\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.write_html('sent_umap_interactive.html')\n",
    "fig.show()\n",
    "print('sent_umap_interactive.html 저장')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 문서 수준 집계 (문장 임베딩 → 논문 임베딩)\n",
    "\n",
    "각 논문에 속한 문장 임베딩의 **평균**을 계산하여 논문 단위 표현을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 논문별 문장 임베딩 평균 (mean pooling)\n",
    "doc_embed_rows = []\n",
    "for doc_idx, grp in df_sent.groupby('doc_idx'):\n",
    "    idxs = grp.index.tolist()\n",
    "    mean_vec = sent_embeddings[idxs].mean(axis=0)\n",
    "    mean_vec /= (np.linalg.norm(mean_vec) + 1e-10)  # 정규화\n",
    "    doc_embed_rows.append({'doc_idx': doc_idx, 'year': grp['year'].iloc[0],\n",
    "                           'n_sents': len(grp)})\n",
    "\n",
    "df_doc = pd.DataFrame(doc_embed_rows)\n",
    "doc_embeddings = np.vstack([\n",
    "    sent_embeddings[df_sent[df_sent['doc_idx'] == r['doc_idx']].index].mean(axis=0)\n",
    "    for _, r in df_doc.iterrows()\n",
    "])\n",
    "\n",
    "print(f'문서 임베딩 shape: {doc_embeddings.shape}')\n",
    "print(f'논문당 평균 문장 수: {df_doc[\"n_sents\"].mean():.1f}')\n",
    "df_doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연도별 문서 임베딩 중심 코사인 유사도 히트맵\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "year_centroids = {}\n",
    "for year, grp in df_doc.groupby('year'):\n",
    "    year_centroids[year] = doc_embeddings[grp.index].mean(axis=0)\n",
    "\n",
    "years_sorted = sorted(year_centroids.keys())\n",
    "centroid_matrix = np.array([year_centroids[y] for y in years_sorted])\n",
    "centroid_norm = centroid_matrix / np.linalg.norm(centroid_matrix, axis=1, keepdims=True)\n",
    "sim_matrix = cosine_similarity(centroid_norm)\n",
    "\n",
    "sim_df = pd.DataFrame(sim_matrix, index=years_sorted, columns=years_sorted)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "mask = np.eye(len(years_sorted), dtype=bool)\n",
    "sns.heatmap(sim_df, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            vmin=sim_matrix[~mask].min(), vmax=1,\n",
    "            linewidths=0.5, ax=ax, cbar_kws={'label': '코사인 유사도'})\n",
    "ax.set_title('연도별 평균 임베딩 코사인 유사도 (문장 기반)', fontsize=13)\n",
    "ax.set_xlabel('연도'); ax.set_ylabel('연도')\n",
    "plt.xticks(rotation=45); plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sent_year_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('sent_year_heatmap.png 저장')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 유사 문장 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_sentences(query: str, top_k: int = 10, year_filter: list = None):\n",
    "    \"\"\"쿼리 텍스트와 의미적으로 유사한 문장을 반환합니다.\"\"\"\n",
    "    query_embed = model.encode([query], normalize_embeddings=True)\n",
    "    sims = cosine_similarity(query_embed, sent_embeddings)[0]\n",
    "\n",
    "    if year_filter:\n",
    "        mask = df_sent['year'].isin(year_filter).values\n",
    "        sims = np.where(mask, sims, -1)\n",
    "\n",
    "    top_idx = sims.argsort()[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for rank, idx in enumerate(top_idx, 1):\n",
    "        if sims[idx] < 0:\n",
    "            break\n",
    "        results.append({\n",
    "            '순위': rank,\n",
    "            '유사도': round(float(sims[idx]), 4),\n",
    "            '연도': int(df_sent.iloc[idx]['year']),\n",
    "            '문장': df_sent.iloc[idx]['sentence']\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 예시 검색\n",
    "query_text = \"인공지능을 활용한 영어 교육\"\n",
    "print(f'검색어: \"{query_text}\"')\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "find_similar_sentences(query_text, top_k=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연도를 지정한 검색 예시\n",
    "query_text2 = \"챗봇 기반 언어 학습\"\n",
    "print(f'검색어: \"{query_text2}\"  (2020년 이후만)')\n",
    "find_similar_sentences(query_text2, top_k=7, year_filter=[2020, 2021, 2022, 2023, 2024, 2025])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 단위 결과\n",
    "out_cols = ['doc_idx', 'sent_idx', 'year', 'cluster', 'umap_x', 'umap_y', 'sentence']\n",
    "df_sent[out_cols].to_csv('sent_cluster_result.csv', index=False, encoding='utf-8-sig')\n",
    "print('sent_cluster_result.csv 저장')\n",
    "\n",
    "# 클러스터 요약\n",
    "summary = []\n",
    "for c in valid_clusters:\n",
    "    mask = df_sent['cluster'] == c\n",
    "    summary.append({\n",
    "        '클러스터': c,\n",
    "        '문장수': int(mask.sum()),\n",
    "        '논문수': int(df_sent[mask]['doc_idx'].nunique()),\n",
    "        '주요연도': int(df_sent[mask]['year'].mode().iloc[0]),\n",
    "        '키워드': ' | '.join(cluster_keywords.get(c, [])[:8])\n",
    "    })\n",
    "pd.DataFrame(summary).to_csv('sent_cluster_summary.csv', index=False, encoding='utf-8-sig')\n",
    "print('sent_cluster_summary.csv 저장')\n",
    "\n",
    "print()\n",
    "print('=== 분석 완료 ===')\n",
    "for f in ['sent_embeddings_all.npy', 'sent_meta_all.csv', 'sent_umap_2d_all.npy',\n",
    "          'sent_umap_visualization.png', 'sent_umap_interactive.html',\n",
    "          'sent_year_heatmap.png', 'sent_cluster_result.csv', 'sent_cluster_summary.csv']:\n",
    "    exists = 'O' if Path(f).exists() else 'X'\n",
    "    print(f'  [{exists}] {f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
